{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset,SubsetRandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve,f1_score,roc_curve,roc_auc_score,auc,accuracy_score,average_precision_score,precision_score,recall_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        # torch.cuda.manual_seed_all(seed)  # 如果使用多个GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_report(y_tru, y_prd, mode, learning_rate, batch_size,epochs, figsize=(7, 7), ax=None):\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    xticks = ['precision', 'recall', 'f1-score', 'support']\n",
    "    yticks = [\"Control\", \"Moderate\", \"Alzheimer's\" ] \n",
    "    yticks += ['avg']\n",
    "\n",
    "    rep = np.array(precision_recall_fscore_support(y_tru, y_prd)).T\n",
    "    avg = np.mean(rep, axis=0)\n",
    "    avg[-1] = np.sum(rep[:, -1])\n",
    "    rep = np.insert(rep, rep.shape[0], avg, axis=0)\n",
    "\n",
    "    sns.heatmap(rep,\n",
    "                annot=True, \n",
    "                cbar=False, \n",
    "                xticklabels=xticks, \n",
    "                yticklabels=yticks,\n",
    "                ax=ax, cmap = \"Blues\")\n",
    "    \n",
    "    plt.savefig('report_' + str(mode) + '_' + str(learning_rate) +'_' + str(batch_size)+'_' + str(epochs)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_confusion_matrix(result, test_label,mode, learning_rate, batch_size, epochs):\n",
    "    result = F.one_hot(result,num_classes=4)\n",
    "    # print(result)\n",
    "\n",
    "    test_label = F.one_hot(test_label,num_classes=4)\n",
    "    # print(test_label)\n",
    "\n",
    "    true_label= np.argmax(test_label, axis =1)\n",
    "\n",
    "    predicted_label= np.argmax(result, axis =1)\n",
    "    \n",
    "    n_classes = 4\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    thres = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], thres[i] = precision_recall_curve(test_label[:, i],\n",
    "                                                            result[:, i])\n",
    "\n",
    "\n",
    "    print (\"Classification Report :\") \n",
    "    print (classification_report(true_label, predicted_label))\n",
    "    cr = classification_report(true_label, predicted_label, output_dict=True)\n",
    "    return cr, precision, recall, thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout_r=0., use_relu=True):\n",
    "        super(FC, self).__init__()\n",
    "        self.dropout_r = dropout_r\n",
    "        self.use_relu = use_relu\n",
    "\n",
    "        self.linear = nn.Linear(in_size, out_size)\n",
    "\n",
    "        if use_relu:\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if dropout_r > 0:\n",
    "            self.dropout = nn.Dropout(dropout_r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "\n",
    "        if self.use_relu:\n",
    "            x = self.relu(x)\n",
    "\n",
    "        if self.dropout_r > 0:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_size, mid_size, out_size, dropout_r=0., use_relu=True):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc = FC(in_size, mid_size, dropout_r=dropout_r, use_relu=use_relu)\n",
    "        self.linear = nn.Linear(mid_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.fc(x))\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(FFN, self).__init__()\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            in_size=50,\n",
    "            mid_size=100,\n",
    "            out_size=50,\n",
    "            dropout_r=0.5,\n",
    "            use_relu=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,input,num_classes=4):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mode,batch_size, epochs, learning_rate, seed):\n",
    "\n",
    "    X_train = pd.read_csv(\"../../processed_data/PPMI/Transcriptomics/X_train.csv\").drop(\"PATNO Visit\", axis=1).values\n",
    "    X_val = pd.read_csv(\"../../processed_data/PPMI/overlap/X_val_transpot.csv\").drop(\"PATNO Visit\", axis=1).values\n",
    "    X_test = pd.read_csv(\"../../processed_data/PPMI/overlap/X_test_transpot.csv\").drop(\"PATNO Visit\", axis=1).values\n",
    "\n",
    "    y_train = pd.read_csv(\"../../processed_data/PPMI/Transcriptomics/y_train.csv\").drop(\"PATNO Visit\", axis=1).values.astype(\"int\").flatten()\n",
    "    y_val = pd.read_csv(\"../../processed_data/PPMI/overlap/y_val.csv\").drop(\"PATNO Visit\", axis=1).values.astype(\"int\").flatten()\n",
    "    y_test = pd.read_csv(\"../../processed_data/PPMI/overlap/y_test.csv\").drop(\"PATNO Visit\", axis=1).values.astype(\"int\").flatten()\n",
    "    print(X_train.shape,X_test.shape)\n",
    "\n",
    "    \n",
    "    train_transpot_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    train_label_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    val_transpot_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    val_label_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    test_transpot_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    test_label_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "    train_dataset = TensorDataset(train_transpot_tensor, train_label_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=30, shuffle=True)\n",
    "    val_dataset = TensorDataset(val_transpot_tensor, val_label_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=30, shuffle=False)\n",
    "    test_dataset = TensorDataset(test_transpot_tensor, test_label_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=30, shuffle=False)\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=torch.unique(torch.tensor(y_train)).numpy(), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    model = NeuralNetwork(X_train.shape[1],num_classes=4).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    best_val_acc=0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predicted_label=[]\n",
    "        true_label=[]\n",
    "        for transpot, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(transpot)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            predicted_label.extend(predicted.tolist())\n",
    "            true_label.extend(labels.tolist())\n",
    "        # scheduler.step()\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        # train_acc = 100.0 * correct / total\n",
    "        train_acc = f1_score(true_label,predicted_label,average='macro')\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predicted_label=[]\n",
    "        true_label=[]\n",
    "        for transpot, labels in val_loader:\n",
    "            outputs = model(transpot)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            predicted_label.extend(predicted.tolist())\n",
    "            true_label.extend(labels.tolist())\n",
    "        # val_acc = 100.0 * correct / total\n",
    "        val_acc = f1_score(true_label,predicted_label,average='macro')\n",
    "        print(f\"Epoch {epoch+1}, train Loss: {train_loss},train acc:{train_acc},val acc:{val_acc},best acc:{best_val_acc}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), f'../../models/PPMI/Transcriptomics/best_{seed}.pth')\n",
    "            print(f'Epoch {epoch+1} get best modal')\n",
    "    \n",
    "    best_model.eval()\n",
    "    # all_probs=[]\n",
    "    predicted_label=[]\n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(test_transpot_tensor)\n",
    "        # probs = nn.functional.softmax(outputs, dim=1)\n",
    "        # all_probs.extend(probs.cpu().detach().numpy())\n",
    "        _, predicted = outputs.max(1)\n",
    "        predicted_label.extend(predicted.tolist())\n",
    "    # test_acc = 100.0 * correct / total\n",
    "    test_acc = f1_score(y_test,predicted_label,average='macro')\n",
    "    # 计算AUC\n",
    "    num_classes=4\n",
    "    true_label_binarized = label_binarize(y_test, classes=list(range(num_classes)))\n",
    "    # predicted_label_binarized = label_binarize(predicted_label, classes=list(range(num_classes)))\n",
    "    auc_score = roc_auc_score(true_label_binarized, outputs.cpu().detach().numpy(), average='macro', multi_class='ovr')\n",
    "    \n",
    "    print(f'test Acc: {test_acc:.4f},test auc: {auc_score:.4f}')\n",
    "    print(predicted_label)\n",
    "    cr, precision, recall, thresholds = calc_confusion_matrix(torch.tensor(predicted_label), test_label_tensor.cpu(), mode, learning_rate, batch_size, epochs)\n",
    "    \n",
    "    return cr , batch_size, learning_rate, epochs, seed,auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurancy=[]\n",
    "precision=[]\n",
    "recall=[]\n",
    "f1=[]\n",
    "auc_score_list=[]\n",
    "seeds = random.sample(range(1, 200), 5)\n",
    "for s in seeds:\n",
    "    set_random_seed(s)\n",
    "    print('seeds:',s)\n",
    "    cr, bs_, lr_, e_ , seed,auc_score= train('Transpot', 30, 1000, 0.0001, s)\n",
    "    accurancy.append(cr['accuracy'])\n",
    "    precision.append(cr[\"macro avg\"][\"precision\"])\n",
    "    recall.append(cr[\"macro avg\"][\"recall\"])\n",
    "    f1.append(cr[\"macro avg\"][\"f1-score\"])\n",
    "    auc_score_list.append(auc_score)\n",
    "    print ('-'*55)\n",
    "print(\"Mean accuracy is: \",sum(accurancy)/len(accurancy))\n",
    "print(\"precision:\",sum(precision)/len(precision))\n",
    "print(\"recall:\",sum(recall)/len(recall))\n",
    "print(\"f1:\",sum(f1)/len(f1))\n",
    "print(\"auc_score:\",sum(auc_score_list)/len(auc_score_list))\n",
    "print(\"Std accuracy: \" + str(np.array(accurancy).std()))\n",
    "print(\"Std precision: \" + str(np.array(precision).std()))\n",
    "print(\"Std recall: \" + str(np.array(recall).std()))\n",
    "print(\"Std f1: \" + str(np.array(f1).std()))\n",
    "print(\"Std auc_score: \" + str(np.array(auc_score_list).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_scaler = StandardScaler()\n",
    "test_transcriptomics = pd.read_csv(\"../../processed_data/PPMI/overlap/X_test_transpot.csv\").drop(\"PATNO Visit\", axis=1).values\n",
    "# test_img = make_img(\"../../processed_data/PPMI/overlap/X_test_img.pkl\")\n",
    "test_label = pd.read_csv(\"../../processed_data/PPMI/overlap/y_test.csv\").drop(\"PATNO Visit\", axis=1).values.astype(\"int\").flatten()\n",
    "test_transcriptomics_tensor = torch.tensor(test_transcriptomics, dtype=torch.float).to(device)\n",
    "# test_img_tensor = torch.tensor(test_img, dtype=torch.float).to(device)\n",
    "\n",
    "modal_list=[13,144,146,178,185]\n",
    "print('f1_score                 acc                 precision           recall              auc             aupr')\n",
    "accurancy=[]\n",
    "precision=[]\n",
    "recall=[]\n",
    "f1=[]\n",
    "auc_score_list=[]\n",
    "aupr_score_list=[]\n",
    "for i in modal_list:\n",
    "    img_modal=NeuralNetwork(test_transcriptomics.shape[1]).to(device)\n",
    "    img_modal.load_state_dict(torch.load(f'../../models/PPMI/Transcriptomics/best_{i}.pth'))\n",
    "    img_modal.eval()\n",
    "    predicted_label_0=[]\n",
    "    with torch.no_grad():\n",
    "        outputs_0 = img_modal(test_transcriptomics_tensor)\n",
    "        # probs_0 = nn.functional.softmax(outputs_0, dim=1)\n",
    "        # all_probs_0.extend(probs_0.cpu().detach().numpy())\n",
    "        _, predicted_0 = outputs_0.max(1)\n",
    "        predicted_label_0.extend(predicted_0.tolist())\n",
    "\n",
    "    num_classes=4\n",
    "    true_label_binarized = label_binarize(test_label, classes=list(range(num_classes)))\n",
    "    test_acc_0=accuracy_score(test_label,predicted_label_0)\n",
    "    test_f1_0 = f1_score(test_label,predicted_label_0,average='macro')\n",
    "    test_precision_0=precision_score(test_label,predicted_label_0,average='macro')\n",
    "    recall_0=recall_score(test_label,predicted_label_0,average='macro')\n",
    "    auc_score_0 = roc_auc_score(true_label_binarized, outputs_0.cpu().detach().numpy(), average='macro', multi_class='ovr')\n",
    "    aupr_score_0 = average_precision_score(true_label_binarized, outputs_0.cpu().detach().numpy(), average='macro')\n",
    "    print(test_f1_0,test_acc_0,test_precision_0,recall_0,auc_score_0,aupr_score_0)\n",
    "    accurancy.append(test_acc_0)\n",
    "    precision.append(test_precision_0)\n",
    "    recall.append(recall_0)\n",
    "    f1.append(test_f1_0)\n",
    "    auc_score_list.append(auc_score_0)\n",
    "    aupr_score_list.append(aupr_score_0)\n",
    "print('avg')\n",
    "print(sum(f1)/len(modal_list),sum(accurancy)/len(modal_list),sum(precision)/len(modal_list),sum(recall)/len(modal_list),sum(auc_score_list)/len(modal_list),sum(aupr_score_list)/len(modal_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MOGONET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
